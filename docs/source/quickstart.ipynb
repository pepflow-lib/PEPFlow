{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14486af",
   "metadata": {},
   "source": [
    "# Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722da191",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "<span class=\"brand-color\">PEPFlow</span> supports version 3.10 or newer. To install directly, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a89d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pepflow --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587e96cc",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "After installation, letâ€™s run our first example with <span class=\"brand-color\">PEPFlow</span>. We begin by importing the required libraries and creating a <span class=\"code-color\">PEP context</span>, which serves as the container for the problem instance and manages all elements of the PEP framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca25d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pepflow as pf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ctx = pf.PEPContext(\"gd\")\n",
    "pf.set_current_context(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2266869b",
   "metadata": {},
   "source": [
    "Next, we set up the performance estimation problem (PEP). We declare the four ingredients of algorithm analysis:\n",
    "- <span style=\"font-weight:bold\">function class</span>: $L$-smooth convex functions;\n",
    "- <span style=\"font-weight:bold\">algorithm of interest</span>: gradient descent method;\n",
    "- <span style=\"font-weight:bold\">initial condition</span>: initial point and optimum are not too far away $\\|x_0 - x_\\star\\|^2 \\leq 1$;\n",
    "- <span style=\"font-weight:bold\">performance metric</span>: objective gap $f(x_k) - f(x_\\star)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf05430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pep_builder = pf.PEPBuilder(ctx)\n",
    "f = pf.SmoothConvexFunction(is_basis=True, tags=[\"f\"], L=1)\n",
    "x_star = f.set_stationary_point(\"x_star\")\n",
    "x = pf.Vector(is_basis=True, tags=[\"x_0\"])  # Initial point\n",
    "pep_builder.add_initial_constraint(((x - x_star) ** 2).le(1, name=\"initial_condition\"))\n",
    "\n",
    "# Define the gradient descent method\n",
    "N = 8\n",
    "for i in range(N):\n",
    "    x = x - f.grad(x)\n",
    "    x.add_tag(f\"x_{i + 1}\")\n",
    "\n",
    "opt_values = []\n",
    "for k in range(1, N):\n",
    "    x_k = ctx[f\"x_{k}\"]\n",
    "    pep_builder.set_performance_metric(f(x_k) - f(x_star))\n",
    "    result = pep_builder.solve()\n",
    "    opt_values.append(result.opt_value)\n",
    "\n",
    "plt.scatter(range(1, N), opt_values, color=\"blue\", marker=\"o\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1603665e",
   "metadata": {},
   "source": [
    "Each scatter point gives an upper bound on the objective gap that holds for all $L$-smooth convex functions after the corresponding number of gradient descent steps; such convergence guarantees are known as <span style=\"font-weight:bold\">worst-case</span> bounds.\n",
    "\n",
    "We compare the numerical values with the analytical rate:\n",
    "\n",
    "$$\n",
    "f(x_k) - f(x_\\star) \\leq \\frac{L}{4k+2} \\|x_0 - x_\\star\\|_2^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12073a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = np.arange(1, N)\n",
    "cont_iters = np.arange(1, N, 0.01)\n",
    "plt.plot(\n",
    "    cont_iters,\n",
    "    1 / (4 * cont_iters + 2),\n",
    "    \"r-\",\n",
    "    label=\"Analytical bound $\\\\frac{1}{4k+2}$\",\n",
    ")\n",
    "plt.scatter(iters, opt_values, color=\"blue\", marker=\"o\", label=\"Numerical values\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73bc4b0",
   "metadata": {},
   "source": [
    "The numerical results from <span class=\"brand-color\">PEPFlow</span> match the theoretical convergence rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3793ea8",
   "metadata": {},
   "source": [
    "## Congratulations\n",
    "\n",
    "You've just run your first <span class=\"brand-color\">PEPFlow</span> example!\n",
    "\n",
    "To explore <span class=\"brand-color\">PEPFlow</span> further, users are encouraged to begin with the [Tutorial](tutorial.md), which introduces the PEP theory and showcases how <span class=\"brand-color\">PEPFlow</span> helps derive analytical proofs corresponding to the numerical results shown above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pepflow (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
